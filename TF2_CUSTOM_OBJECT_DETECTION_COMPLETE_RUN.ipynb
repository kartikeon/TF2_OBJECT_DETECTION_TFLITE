{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EDTwti22stx"
   },
   "source": [
    "#Python Version Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d84Bbqht1cM4"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZM6DDem2BC4"
   },
   "outputs": [],
   "source": [
    "# !sudo apt-get update -y\n",
    "# !sudo apt-get install python3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkQtAiNXuwGB"
   },
   "outputs": [],
   "source": [
    "#change alternatives\n",
    "# !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
    "# !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thGUGUc6wUyv"
   },
   "outputs": [],
   "source": [
    "# !sudo apt install python3-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHVVJzopuzwI"
   },
   "outputs": [],
   "source": [
    "# !python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1Khqkvc7af2"
   },
   "source": [
    "# Step-(0): Mount the Drive to colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgHxuNvpAUDF"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZpNSGBRjnhj"
   },
   "outputs": [],
   "source": [
    "# For removing the default sample data in drive.\n",
    "!rm -r '/content/sample_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LAysJaf5u9k"
   },
   "source": [
    "# Step-(1) : Import the Image Dataset from the Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjQiSLSFN70p"
   },
   "outputs": [],
   "source": [
    "# To Unzip the dataset zip file\n",
    "!unzip /content/drive/MyDrive/ML-dir/Datasets/rpgo_v1_datasets.zip -d '/content'\n",
    "\n",
    "# To remove the dir\n",
    "# !rm -r '/content/rpgo_imgsets_colab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcO-WUmh662_"
   },
   "source": [
    "# Step-(2): Install the Pre-requisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDcVJjyiudlI"
   },
   "outputs": [],
   "source": [
    "!pip uninstall Cython -y # Temporary fix for \"No module named 'object_detection'\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMEGqJ2SV-AA"
   },
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository from GitHub\n",
    "!git clone --depth 1 https://github.com/tensorflow/models\n",
    "# !rm -r '/content/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nClmtmf9V_MJ"
   },
   "outputs": [],
   "source": [
    "# Copy setup files into models/research folder\n",
    "%%bash\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "#cp object_detection/packages/tf2/setup.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udG3ReJHWHM_"
   },
   "outputs": [],
   "source": [
    "# Modify setup.py file to install the tf-models-official repository targeted at TF v2.8.0\n",
    "import re\n",
    "with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n",
    "    s = f.read()\n",
    "\n",
    "with open('/content/models/research/setup.py', 'w') as f:\n",
    "    # Set fine_tune_checkpoint path\n",
    "    s = re.sub('tf-models-official>=2.5.1',\n",
    "               'tf-models-official==2.8.0', s)\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlsSlngvWKji"
   },
   "outputs": [],
   "source": [
    "# Install the Object Detection API\n",
    "# Need to do a temporary fix with PyYAML because Colab isn't able to install PyYAML v5.4.1\n",
    "!pip install pyyaml==5.3\n",
    "!pip install /content/models/research/\n",
    "\n",
    "# Need to downgrade to TF v2.8.0 due to Colab compatibility bug with TF v2.10 (as of 10/03/22)\n",
    "!pip install tensorflow==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EaLITreWSHp"
   },
   "outputs": [],
   "source": [
    "# Run Model Bulider Test file, just to verify everything's working properly\n",
    "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxlobQan3iQ5"
   },
   "source": [
    "# Step-(3): Set the Path for TF-Record dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3mOj0LdQlYJ"
   },
   "outputs": [],
   "source": [
    "train_record_fname = '/content/rpgo_v1_datasets/rpgo_slant_straight_320_tfrecord/train/train.tfrecord'\n",
    "val_record_fname = '/content/rpgo_v1_datasets/rpgo_slant_straight_320_tfrecord/valid/valid.tfrecord'\n",
    "label_map_pbtxt_fname = '/content/rpgo_v1_datasets/rpgo_slant_straight_320_tfrecord/train/label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6BxLPbh8zt2"
   },
   "source": [
    "# Step-(4): Set the Model Configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9xnzNs8Q_eQ"
   },
   "outputs": [],
   "source": [
    "# Change the chosen_model variable to deploy different models available in the TF2 object detection zoo\n",
    "chosen_model = 'ssd-mobilenet-v2-fpnlite-320'\n",
    "\n",
    "MODELS_CONFIG = {\n",
    "    'ssd-mobilenet-v2': {\n",
    "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
    "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
    "    },\n",
    "    'ssd-mobilenet-v2-fpnlite-320': {\n",
    "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
    "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n",
    "    },\n",
    "    'efficientdet-d0': {\n",
    "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
    "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
    "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
    "\n",
    "    },\n",
    "    # The centernet model isn't working as of 9/10/22\n",
    "    #'centernet-mobilenet-v2': {\n",
    "    #    'model_name': 'centernet_mobilenetv2fpn_512x512_coco17_od',\n",
    "    #    'base_pipeline_file': 'pipeline.config',\n",
    "    #    'pretrained_checkpoint': 'centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz',\n",
    "    #}\n",
    "}\n",
    "\n",
    "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
    "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
    "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7L7z1Z4yRAsV"
   },
   "outputs": [],
   "source": [
    "# Create \"mymodel\" folder for holding pre-trained weights and configuration files\n",
    "%mkdir /content/models/mymodel/\n",
    "%cd /content/models/mymodel/\n",
    "\n",
    "# Download pre-trained model weights\n",
    "import tarfile\n",
    "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
    "!wget {download_tar}\n",
    "tar = tarfile.open(pretrained_checkpoint)\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# Download training configuration file for model\n",
    "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
    "!wget {download_config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siQrwpBY9ylR"
   },
   "source": [
    "# Step-(5): Set the Training steps for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUWR-YLORRcO"
   },
   "outputs": [],
   "source": [
    "# Set training parameters for the model\n",
    "\n",
    "# Set the training steps.\n",
    "num_steps = 1000\n",
    "\n",
    "if chosen_model == 'efficientdet-d0':\n",
    "  batch_size = 4\n",
    "else:\n",
    "  batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lv05OJCPRSOv"
   },
   "outputs": [],
   "source": [
    "# Set file locations and get number of classes for config file\n",
    "pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n",
    "fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n",
    "\n",
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())\n",
    "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
    "print('Total classes:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaDHhGtVRVNH"
   },
   "outputs": [],
   "source": [
    "# Create custom configuration file by writing the dataset, model checkpoint, and training parameters into the base pipeline file\n",
    "import re\n",
    "\n",
    "%cd /content/models/mymodel\n",
    "print('writing custom configuration file')\n",
    "\n",
    "with open(pipeline_fname) as f:\n",
    "    s = f.read()\n",
    "with open('pipeline_file.config', 'w') as f:\n",
    "\n",
    "    # Set fine_tune_checkpoint path\n",
    "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
    "\n",
    "    # Set tfrecord files for train and test datasets\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
    "\n",
    "    # Set label_map_path\n",
    "    s = re.sub(\n",
    "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
    "\n",
    "    # Set batch_size\n",
    "    s = re.sub('batch_size: [0-9]+',\n",
    "               'batch_size: {}'.format(batch_size), s)\n",
    "\n",
    "    # Set training steps, num_steps\n",
    "    s = re.sub('num_steps: [0-9]+',\n",
    "               'num_steps: {}'.format(num_steps), s)\n",
    "\n",
    "    # Set number of classes num_classes\n",
    "    s = re.sub('num_classes: [0-9]+',\n",
    "               'num_classes: {}'.format(num_classes), s)\n",
    "\n",
    "    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n",
    "    s = re.sub(\n",
    "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
    "\n",
    "    # If using ssd-mobilenet-v2, reduce learning rate (because it's too high in the default config file)\n",
    "    if chosen_model == 'ssd-mobilenet-v2':\n",
    "      s = re.sub('learning_rate_base: .8',\n",
    "                 'learning_rate_base: .08', s)\n",
    "\n",
    "      s = re.sub('warmup_learning_rate: 0.13333',\n",
    "                 'warmup_learning_rate: .026666', s)\n",
    "\n",
    "    # If using efficientdet-d0, use fixed_shape_resizer instead of keep_aspect_ratio_resizer (because it isn't supported by TFLite)\n",
    "    if chosen_model == 'efficientdet-d0':\n",
    "      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n",
    "      s = re.sub('pad_to_max_dimension: true', '', s)\n",
    "      s = re.sub('min_dimension', 'height', s)\n",
    "      s = re.sub('max_dimension', 'width', s)\n",
    "\n",
    "    f.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-iJR4sQRb5_"
   },
   "outputs": [],
   "source": [
    "# Set the path to the custom config file and the directory to store training checkpoints in\n",
    "pipeline_file = '/content/models/mymodel/pipeline_file.config'\n",
    "model_dir = '/content/training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8ZgUPKvRi2v"
   },
   "source": [
    "# Step-(6): Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILW3rKdHRriv"
   },
   "outputs": [],
   "source": [
    "# Run training!\n",
    "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path={pipeline_file} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={num_steps} \\\n",
    "    --sample_1_of_n_eval_examples=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwNEubA3Rxun"
   },
   "source": [
    "# Step-(7): Convert Model to TensorFlow SavedModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmhIWhelR4Vw"
   },
   "outputs": [],
   "source": [
    "# Make a directory to store the trained TFLite model\n",
    "!mkdir /content/custom_model_lite\n",
    "output_directory = '/content/custom_model_lite'\n",
    "\n",
    "# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\n",
    "last_model_path = '/content/training'\n",
    "\n",
    "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
    "    --trained_checkpoint_dir {last_model_path} \\\n",
    "    --output_directory {output_directory} \\\n",
    "    --pipeline_config_path {pipeline_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1-V2bEhQgx-"
   },
   "outputs": [],
   "source": [
    "# !rm -r '/content/custom_model_lite'\n",
    "# !rm -r '/content/models'\n",
    "# !rm -r '/content/training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nc6zPWrCvbdw"
   },
   "source": [
    "# Step-(8): Convert SavedModel to Quantized TF-lite Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTpMe3BWX9ex"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zu07jSi-iMLp"
   },
   "source": [
    "Ordinarily, creating a TensorFlow Lite model is just a few lines of code with [`TFLiteConverter`](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter). For example, this creates a basic (un-quantized) TensorFlow Lite model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1EimtDOR8E3"
   },
   "outputs": [],
   "source": [
    "# To convert a basic (un-quantized) TensorFlow Lite model:\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# with open('/content/custom_model_lite/-320-normal.tflite', 'wb') as f:\n",
    "  # f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aucwgRGBiJ2L"
   },
   "source": [
    "However, this `.tflite` file still uses floating-point values for the parameter data, and we need to fully quantize the model to int8 format.\n",
    "\n",
    "To fully quantize the model, we need to perform [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) with a representative dataset, which requires a few more arguments for the `TFLiteConverter`, and a function that builds a dataset that's representative of the training dataset.\n",
    "\n",
    "So let's convert the model again with post-training quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf9Crpk0IVCx"
   },
   "outputs": [],
   "source": [
    "# Get list of all images in train directory\n",
    "image_path = '/content/rpgo_v1_datasets/rpgo_slant_straight_320_yolov8/train/images'\n",
    "\n",
    "jpg_file_list = glob.glob(image_path + '/*.jpg')\n",
    "JPG_file_list = glob.glob(image_path + '/*.JPG')\n",
    "png_file_list = glob.glob(image_path + '/*.png')\n",
    "bmp_file_list = glob.glob(image_path + '/*.bmp')\n",
    "\n",
    "quant_image_list = jpg_file_list + JPG_file_list + png_file_list + bmp_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeYweZ60IVwI"
   },
   "outputs": [],
   "source": [
    "imgsz = 320\n",
    "\n",
    "def representative_data_gen():\n",
    "  dataset_list = quant_image_list\n",
    "  quant_num = 300\n",
    "  for i in range(quant_num):\n",
    "    pick_me = random.choice(dataset_list)\n",
    "    image = tf.io.read_file(pick_me)\n",
    "\n",
    "    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG'):\n",
    "      image = tf.io.decode_jpeg(image, channels=3)\n",
    "    elif pick_me.endswith('.png'):\n",
    "      image = tf.io.decode_png(image, channels=3)\n",
    "    elif pick_me.endswith('.bmp'):\n",
    "      image = tf.io.decode_bmp(image, channels=3)\n",
    "\n",
    "    image = tf.image.resize(image, [imgsz, imgsz])  # TO DO: Replace 300s with an automatic way of reading network input size\n",
    "    image = tf.cast(image / 255., tf.float32)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    yield [image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6qM6LkfIX4R"
   },
   "outputs": [],
   "source": [
    "# Initialize converter module\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n",
    "\n",
    "# This enables quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# This sets the representative dataset for quantization\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# This ensures that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
    "# converter.target_spec.supported_types = [tf.int8]\n",
    "# These set the input tensors to uint8 and output tensors to float32\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('/content/custom_model_lite/rpgo-slant-straight-320-int8.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ffkqC67dOB7"
   },
   "outputs": [],
   "source": [
    "# To Copy the tflite model to drive\n",
    "# !scp -r '/content/custom_model_lite/mobilenet-v2-640-normal.tflite' '/content/drive/MyDrive/rpgo_colab_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5AOaVZx4nUZ"
   },
   "source": [
    "# Step-(9): Metadata Writer for TF-lite Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-p6mkQAq5CNI"
   },
   "outputs": [],
   "source": [
    "!pip install tflite-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwKtpVOC4moQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tflite_support import flatbuffers\n",
    "from tflite_support import metadata as _metadata\n",
    "from tflite_support import metadata_schema_py_generated as _metadata_fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56Yd6C4Q5xVX"
   },
   "outputs": [],
   "source": [
    "# Creating new metadata dir and copying the model from custom_model_lite dir.\n",
    "!mkdir /content/Metadata_Written_Tf-lite_Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vpLvPdm6PYr"
   },
   "outputs": [],
   "source": [
    "# Copying model to metadata folder\n",
    "!scp -r '/content/custom_model_lite/rpgo-slant-straight-320-int8.tflite' '/content/Metadata_Written_Tf-lite_Models/rpgo-slant-straight-320-int8.tflite-md.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvZCMtMg48tO"
   },
   "outputs": [],
   "source": [
    "model_path = \"/content/Metadata_Written_Tf-lite_Models/rpgo-slant-straight-320-int8.tflite-md.tflite\"\n",
    "label_path = \"/content/rpgo_v1_datasets/label_rpgo.txt\"\n",
    "\n",
    "# Creates model info.\n",
    "model_meta = _metadata_fb.ModelMetadataT()\n",
    "model_meta.name = \"MobileNetV2-FPN-lite Object Detection\"\n",
    "model_meta.description = (\"Identify the most prominent object in the \"\n",
    "                          \"image from a set of 1 categories such as \"\n",
    "                          \"rpgo.\")\n",
    "model_meta.version = \"v2\"\n",
    "model_meta.author = \"TensorFlow\"\n",
    "model_meta.license = \"Apache Version 2.0\"\n",
    "\n",
    "# Creates input info.\n",
    "input_meta = _metadata_fb.TensorMetadataT()\n",
    "\n",
    "# Creates output info.\n",
    "output_meta = _metadata_fb.TensorMetadataT()\n",
    "\n",
    "input_meta.name = \"image\"\n",
    "input_meta.description = (\n",
    "    \"Input image to be classified. The expected image is {0} x {1}, with \"\n",
    "    \"three channels (red, blue, and green) per pixel. Each value in the \"\n",
    "    \"tensor is a single byte between 0 and 255.\".format(320, 320))\n",
    "input_meta.content = _metadata_fb.ContentT()\n",
    "input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n",
    "input_meta.content.contentProperties.colorSpace = (\n",
    "    _metadata_fb.ColorSpaceType.RGB)\n",
    "input_meta.content.contentPropertiesType = (\n",
    "    _metadata_fb.ContentProperties.ImageProperties)\n",
    "input_normalization = _metadata_fb.ProcessUnitT()\n",
    "input_normalization.optionsType = (\n",
    "    _metadata_fb.ProcessUnitOptions.NormalizationOptions)\n",
    "input_normalization.options = _metadata_fb.NormalizationOptionsT()\n",
    "input_normalization.options.mean = [127.5]\n",
    "input_normalization.options.std = [127.5]\n",
    "input_meta.processUnits = [input_normalization]\n",
    "input_stats = _metadata_fb.StatsT()\n",
    "input_stats.max = [255]\n",
    "input_stats.min = [0]\n",
    "input_meta.stats = input_stats\n",
    "\n",
    "# Creates output info.\n",
    "output_location_meta = _metadata_fb.TensorMetadataT()\n",
    "output_location_meta.name = \"location\"\n",
    "output_location_meta.description = \"The bbox or locations of the detected boxes.\"\n",
    "output_location_meta.content = _metadata_fb.ContentT()\n",
    "output_location_meta.content.contentPropertiesType = (_metadata_fb.ContentProperties.BoundingBoxProperties) # https://www.tensorflow.org/lite/api_docs/python/tflite_support/metadata_schema_py_generated/ContentProperties\n",
    "output_location_meta.content.contentProperties = (_metadata_fb.BoundingBoxPropertiesT())\n",
    "output_location_meta.content.contentProperties.index = [1, 0, 3, 2] # https://www.tensorflow.org/lite/api_docs/python/tflite_support/metadata_schema_py_generated/BoundingBoxType\n",
    "output_location_meta.content.contentProperties.type = (_metadata_fb.BoundingBoxType.BOUNDARIES)\n",
    "output_location_meta.content.contentProperties.coordinateType = (_metadata_fb.CoordinateType.RATIO)\n",
    "output_location_meta.content.range = _metadata_fb.ValueRangeT()\n",
    "output_location_meta.content.range.min = 2\n",
    "output_location_meta.content.range.max = 2\n",
    "\n",
    "output_class_meta = _metadata_fb.TensorMetadataT()\n",
    "output_class_meta.name = \"classes\"\n",
    "output_class_meta.description = \"The class or categories of the detected boxes.\"\n",
    "output_class_meta.content = _metadata_fb.ContentT()\n",
    "output_class_meta.content.contentPropertiesType = (\n",
    "        _metadata_fb.ContentProperties.FeatureProperties)\n",
    "output_class_meta.content.contentProperties = (\n",
    "        _metadata_fb.FeaturePropertiesT())\n",
    "output_class_meta.content.range = _metadata_fb.ValueRangeT()\n",
    "output_class_meta.content.range.min = 2\n",
    "output_class_meta.content.range.max = 2\n",
    "label_file = _metadata_fb.AssociatedFileT()\n",
    "label_file.name = os.path.basename(label_path)\n",
    "label_file.description = \"Label of objects that this model can recognize.\"\n",
    "label_file.type = _metadata_fb.AssociatedFileType.TENSOR_VALUE_LABELS #TENSOR_AXIS_LABELS\n",
    "output_class_meta.associatedFiles = [label_file]\n",
    "\n",
    "output_score_meta = _metadata_fb.TensorMetadataT()\n",
    "output_score_meta.name = \"score\"\n",
    "output_score_meta.description = \"The scores of the detected boxes.\"\n",
    "output_score_meta.content = _metadata_fb.ContentT()\n",
    "output_score_meta.content.contentPropertiesType = (\n",
    "        _metadata_fb.ContentProperties.FeatureProperties)\n",
    "output_score_meta.content.contentProperties = (\n",
    "        _metadata_fb.FeaturePropertiesT())\n",
    "output_score_meta.content.range = _metadata_fb.ValueRangeT()\n",
    "output_score_meta.content.range.min = 2\n",
    "output_score_meta.content.range.max = 2\n",
    "\n",
    "output_no_of_detection_meta = _metadata_fb.TensorMetadataT()\n",
    "output_no_of_detection_meta.name = \"number of detections\"\n",
    "output_no_of_detection_meta.description = \"The number of the detected boxes.\"\n",
    "output_no_of_detection_meta.content = _metadata_fb.ContentT()\n",
    "output_no_of_detection_meta.content.contentPropertiesType = (\n",
    "        _metadata_fb.ContentProperties.FeatureProperties)\n",
    "output_no_of_detection_meta.content.contentProperties = (\n",
    "        _metadata_fb.FeaturePropertiesT())\n",
    "\n",
    "# Creates subgraph info.\n",
    "group = _metadata_fb.TensorGroupT()\n",
    "group.name = \"detection result\"\n",
    "group.tensorNames = [output_location_meta.name, output_class_meta.name, output_score_meta.name]\n",
    "\n",
    "subgraph = _metadata_fb.SubGraphMetadataT()\n",
    "subgraph.inputTensorMetadata = [input_meta]\n",
    "subgraph.outputTensorMetadata = [output_location_meta, output_class_meta, output_score_meta, output_no_of_detection_meta]\n",
    "subgraph.outputTensorGroups = [group]\n",
    "model_meta.subgraphMetadata = [subgraph]\n",
    "\n",
    "b = flatbuffers.Builder(0)\n",
    "b.Finish(\n",
    "    model_meta.Pack(b),\n",
    "    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
    "metadata_buf = b.Output()\n",
    "\n",
    "\"\"\"Populates metadata and label file to the model file.\"\"\"\n",
    "populator = _metadata.MetadataPopulator.with_model_file(model_path)\n",
    "populator.load_metadata_buffer(metadata_buf)\n",
    "populator.load_associated_files([label_path])\n",
    "populator.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWemHjreDuXH"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW6ekxF0Du9f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_mod_path = \"/content/Metadata_Written_Tf-lite_Models/rpgo-slant-straight-320-int8.tflite-md.tflite\"\n",
    "# Load the TFLite model in TFLite Interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path= tf_mod_path)\n",
    "\n",
    "# Resize input shape for dynamic shape model and allocate tensor\n",
    "interpreter.resize_tensor_input(interpreter.get_output_details()[0]['index'], [3, 10])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter#get_input_details\n",
    "# https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter#get_output_details\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_data = interpreter.get_tensor(input_details[0]['index'])\n",
    "output_data = interpreter.get_tensor(output_details[3]['index'])\n",
    "\n",
    "input_dtype = interpreter.get_input_details()[0]['dtype']\n",
    "output_dtype = interpreter.get_output_details()[0]['dtype']\n",
    "\n",
    "input_dtype = interpreter.get_input_details()[0]['dtype']\n",
    "output_dtype = interpreter.get_output_details()[0]['dtype']\n",
    "\n",
    "input_shape = interpreter.get_input_details()[0]['shape']\n",
    "output_shape_0 = interpreter.get_output_details()[0]['shape']\n",
    "output_shape_1 = interpreter.get_output_details()[1]['shape']\n",
    "output_shape_2 = interpreter.get_output_details()[2]['shape']\n",
    "output_shape_3 = interpreter.get_output_details()[3]['shape']\n",
    "\n",
    "\n",
    "\n",
    "# print(input_details)\n",
    "# print(output_details)\n",
    "\n",
    "print(\"Input Data : \", input_dtype, input_shape)\n",
    "print(\"Output Data : \", output_dtype, output_shape_0, output_shape_1, output_shape_2, output_shape_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKHVt2I1dn-s"
   },
   "source": [
    "# Step-(10): Test the TF-lite Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-WI0iBldlqt"
   },
   "outputs": [],
   "source": [
    "# output index\n",
    "# ------------TF1 index --------------------|--------TF2 index-------|\n",
    "# StatefulPartitionedCall:3 = [1, 10, 4] # boxes         # 1\n",
    "# StatefulPartitionedCall:2 = [1, 10]    # classes       # 3\n",
    "# StatefulPartitionedCall:1 = [1, 10]    # scores        # 0\n",
    "# StatefulPartitionedCall:0 = [1]        # count         # 2\n",
    "\n",
    "\n",
    "# Script to run custom TFLite model on test images to detect objects\n",
    "# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import importlib.util\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### Define function for inferencing with TFLite model and displaying results\n",
    "\n",
    "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
    "\n",
    "  # Grab filenames of all images in test folder\n",
    "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
    "\n",
    "  # Load the label map into memory\n",
    "  with open(lblpath, 'r') as f:\n",
    "      labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "  # Load the Tensorflow Lite model into memory\n",
    "  interpreter = Interpreter(model_path=modelpath)\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # Get model details\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "  height = input_details[0]['shape'][1]\n",
    "  width = input_details[0]['shape'][2]\n",
    "\n",
    "  float_input = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "  input_mean = 127.5\n",
    "  input_std = 127.5\n",
    "\n",
    "  # Randomly select test images\n",
    "  images_to_test = random.sample(images, num_test_images)\n",
    "\n",
    "  # Loop over every image and perform detection\n",
    "  for image_path in images_to_test:\n",
    "\n",
    "      # Load image and resize to expected shape [1xHxWx3]\n",
    "      image = cv2.imread(image_path)\n",
    "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      imH, imW, _ = image.shape\n",
    "      image_resized = cv2.resize(image_rgb, (width, height))\n",
    "      input_data = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "      if float_input:\n",
    "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "      # Perform the actual detection by running the model with the image as input\n",
    "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "      interpreter.invoke()\n",
    "\n",
    "      # Retrieve detection results\n",
    "      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n",
    "      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n",
    "      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n",
    "      detections = []\n",
    "\n",
    "      # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "      for i in range(len(scores)):\n",
    "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
    "\n",
    "              # Get bounding box coordinates and draw box\n",
    "              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
    "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
    "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
    "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
    "\n",
    "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "\n",
    "              # Draw label\n",
    "              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
    "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
    "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "\n",
    "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
    "\n",
    "\n",
    "      # All the results have been drawn on the image, now display the image\n",
    "      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(12,16))\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "      # Save detection results in .txt files (for calculating mAP)\n",
    "      elif txt_only == True:\n",
    "\n",
    "        # Get filenames and paths\n",
    "        image_fn = os.path.basename(image_path)\n",
    "        base_fn, ext = os.path.splitext(image_fn)\n",
    "        txt_result_fn = base_fn +'.txt'\n",
    "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
    "\n",
    "        # Write results to text file\n",
    "        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
    "        with open(txt_savepath,'w') as f:\n",
    "            for detection in detections:\n",
    "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExBPQ2rdtrk"
   },
   "outputs": [],
   "source": [
    "# Set up variables for running user's model\n",
    "\n",
    "PATH_TO_MODEL='/content/Metadata_Written_Tf-lite_Models/rpgo-slant-straight-320-int8.tflite-md.tflite' #/content/tflite_cus/detect.tflite'   # Path to .tflite model file\n",
    "PATH_TO_IMAGES='/content/rpgo_v1_datasets/rpgo_slant_straight_320_yolov8/valid/images'   # Path to test images folder\n",
    "PATH_TO_LABELS='/content/rpgo_v1_datasets/label_rpgo.txt'   # Path to labelmap.txt file\n",
    "min_conf_threshold=0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
    "images_to_test = 10   # Number of images to run detection on\n",
    "\n",
    "# Run inferencing function!\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLG9beh9AsfU"
   },
   "source": [
    "# Step-(11): Calculate mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlWarXEZDUqS"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/Cartucho/mAP /content/mAP\n",
    "cd /content/mAP\n",
    "rm input/detection-results/*\n",
    "rm input/ground-truth/*\n",
    "rm input/images-optional/*\n",
    "wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5szFfVxwI3wT"
   },
   "outputs": [],
   "source": [
    "# Add the pascal voc dataset to this code\n",
    "!cp /content/rpgo_v2_datasets/rpgo_v2_320_pvoc/valid/* /content/mAP/input/images-optional # Copy images and xml files\n",
    "!mv /content/mAP/input/images-optional/*.xml /content/mAP/input/ground-truth/  # Move xml files to the appropriate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdjtOUDnK2AA"
   },
   "outputs": [],
   "source": [
    "# Conversion from Pascal VOC xml to txt.\n",
    "!python /content/mAP/scripts/extra/convert_gt_xml.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMaumV-11Et0"
   },
   "outputs": [],
   "source": [
    "# Need to remove existing detection results first\n",
    "# !rm /content/mAP/input/detection-results/*\n",
    "\n",
    "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
    "PATH_TO_IMAGES='/content/mAP/input/images-optional'   # Path to test images folder\n",
    "PATH_TO_MODEL='/content/rpgo-v2-320-int8-md.tflite'   # Path to quantized .tflite model file\n",
    "PATH_TO_LABELS='/content/rpgo_v2_datasets/label_rpgo.txt'   # Path to labelmap.txt file\n",
    "PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n",
    "\n",
    "min_conf_threshold=0.1   # Confidence threshold\n",
    "\n",
    "# Use all the images in the test folder\n",
    "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
    "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
    "\n",
    "# Tell function to just save results and not display images\n",
    "txt_only = True\n",
    "\n",
    "# Run inferencing function!\n",
    "print('Starting inference on %d images...' % images_to_test)\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
    "print('Finished inferencing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIRNp0Af1Et1"
   },
   "outputs": [],
   "source": [
    "%cd '/content/mAP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TDgMBw_1Et1"
   },
   "outputs": [],
   "source": [
    "!python calculate_map_cartucho.py --labels=/content/rpgo_v2_datasets/label_rpgo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTgPAHJu-4Mh"
   },
   "outputs": [],
   "source": [
    "# To Remove the existing files:\n",
    "!rm /content/mAP/input/detection-results/*\n",
    "!rm -r /content/mAP/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBINunWsW4Qa"
   },
   "source": [
    "#Save the Trained Model Checkpoints to Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cDRHoSDa4ZZ"
   },
   "outputs": [],
   "source": [
    "# To Remove the Dir from /content\n",
    "# !rm -r '/content/custom_model_lite'\n",
    "# !rm -r '/content/models'\n",
    "# !rm -r '/content/training'\n",
    "# !rm -r '/content/mobilnetssd-v2-fpn-lite_320_stp_2000'\n",
    "# !rm -r '/content/rpgo-straight_320_stp_1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyWUc-fMXOwU"
   },
   "outputs": [],
   "source": [
    "!mkdir '/content/rpgo-slant-straight-320_stp_1000'\n",
    "!mkdir '/content/rpgo-slant-straight-320_stp_1000/custom_model_lite'\n",
    "!mkdir '/content/rpgo-slant-straight-320_stp_1000/models'\n",
    "!mkdir '/content/rpgo-slant-straight-320_stp_1000/training'\n",
    "!mkdir '/content/rpgo-slant-straight-320_stp_1000/Metadata_Written_Tf-lite_Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVgwETz-Xfp0"
   },
   "outputs": [],
   "source": [
    "!scp -r '/content/models' '/content/rpgo-slant-straight-320_stp_1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CJ6vdJSY18H"
   },
   "outputs": [],
   "source": [
    "!scp -r '/content/training' '/content/rpgo-slant-straight-320_stp_1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0-E6k7wY6XH"
   },
   "outputs": [],
   "source": [
    "!scp -r '/content/custom_model_lite' '/content/rpgo-slant-straight-320_stp_1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlE6eM6DrI1H"
   },
   "outputs": [],
   "source": [
    "!scp -r '/content/Metadata_Written_Tf-lite_Models' '/content/rpgo-slant-straight-320_stp_1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhBa1aHJZXh5"
   },
   "outputs": [],
   "source": [
    "!zip -r '/content/rpgo-slant-straight-320_stp_1000.zip' 'rpgo-slant-straight-320_stp_1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZwmzjZuEqY9"
   },
   "outputs": [],
   "source": [
    "# !mkdir '/content/drive/MyDrive/Pre_trained_Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByOoEeupZko-"
   },
   "outputs": [],
   "source": [
    "!scp -r '/content/rpgo-slant-straight-320_stp_1000.zip' '/content/drive/MyDrive/ML-dir/Pre_trained_Models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddKJpPqPsOu-"
   },
   "source": [
    "# Save it Final Models Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngUHgYYGsM__"
   },
   "outputs": [],
   "source": [
    "!scp -r '/content/Metadata_Written_Tf-lite_Models/rpgo-slant-straight-320-int8-md.tflite' '/content/drive/MyDrive/ML-dir/Final_Models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So-qseIvFakF"
   },
   "source": [
    "# Code Functions:\n",
    "1. **To make new dir**: !mkdir 'path_to_new_dir'\n",
    "2. **Secured Copy and paste**: !scp -r 'path_to_copy_from_dir' 'path_to_copy_to_dir'\n",
    "3. **To Unzip a zip dir**: !unzip 'path_to_zipfolder.zip' -d 'path_to_destination'\n",
    "4. **To Zip the dir**: !zip -r 'path_to_zip_dir_destination' 'path_to_dir_to_be_zipped'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP8/NuLaTkkltEN7rFmBSC3",
   "collapsed_sections": [
    "4EDTwti22stx",
    "r1Khqkvc7af2",
    "5LAysJaf5u9k",
    "xcO-WUmh662_",
    "UxlobQan3iQ5",
    "H6BxLPbh8zt2",
    "siQrwpBY9ylR",
    "D8ZgUPKvRi2v",
    "CwNEubA3Rxun",
    "nc6zPWrCvbdw",
    "-5AOaVZx4nUZ",
    "zKHVt2I1dn-s",
    "vLG9beh9AsfU",
    "uBINunWsW4Qa",
    "ddKJpPqPsOu-"
   ],
   "gpuType": "T4",
   "mount_file_id": "1iwDzIXId4xacnE7tsNGIIma9hIXB2XSx",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
